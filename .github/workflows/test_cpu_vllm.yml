name: Test CPU vLLM

on:
  workflow_dispatch:
    inputs:
      vllm_version:
        description: 'vLLM version to build and test'
        required: false
        default: 'v0.6.4.post1'
        type: string
      force_rebuild:
        description: 'Force rebuild even if cached wheel exists'
        required: false
        default: false
        type: boolean
  push:
    branches:
      - "main"
      - "release-**"
    paths:
      - 'scripts/build-vllm-cpu.sh'
      - 'scripts/vllm-cpu-config.env'
      - '.github/workflows/test_cpu_vllm.yml'
  pull_request:
    branches:
      - "main"
      - "release-**"
    paths:
      - 'scripts/build-vllm-cpu.sh'
      - 'scripts/vllm-cpu-config.env'
      - '.github/workflows/test_cpu_vllm.yml'
  schedule:
    # Run weekly on Sunday at 3:00 AM UTC to catch any upstream issues
    - cron: '0 3 * * 0'

env:
  LC_ALL: en_US.UTF-8

defaults:
  run:
    shell: bash

permissions:
  contents: read

jobs:
  test-cpu-vllm:
    name: "test-cpu-vllm: Python ${{ matrix.python }} on ${{ matrix.platform }}"
    runs-on: "${{ matrix.platform }}"
    strategy:
      fail-fast: false
      matrix:
        python:
          - "3.11"  # Use single Python version to reduce CI time
        platform:
          - "ubuntu-latest"
    
    steps:
      - name: "Harden Runner"
        uses: step-security/harden-runner@6c439dc8bdf85cadbbce9ed30d1c7b959517bc49 # v2.12.2
        with:
          egress-policy: audit

      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          fetch-depth: 0

      - name: Check CPU features
        run: |
          echo "Checking CPU features required for vLLM CPU build..."
          lscpu
          echo "=== CPU Features ==="
          grep -o 'avx[^ ]*' /proc/cpuinfo | sort | uniq || echo "No AVX features found"
          
          if ! grep -q "avx512f" /proc/cpuinfo; then
            echo "::warning::AVX512F not detected. vLLM CPU build may not work on this runner."
            echo "SKIP_VLLM_CPU_TESTS=true" >> $GITHUB_ENV
          else
            echo "AVX512F detected, proceeding with CPU tests."
          fi

      - name: Free disk space
        if: env.SKIP_VLLM_CPU_TESTS != 'true'
        uses: ./.github/actions/free-disk-space

      - name: Setup Python ${{ matrix.python }}
        if: env.SKIP_VLLM_CPU_TESTS != 'true'
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065 # v5.6.0
        with:
          python-version: ${{ matrix.python }}
          cache: pip
          cache-dependency-path: |
            **/pyproject.toml

      - name: Install system dependencies
        if: env.SKIP_VLLM_CPU_TESTS != 'true'
        run: |
          sudo apt-get update -y
          sudo apt-get install -y --no-install-recommends \
            ccache \
            gcc-12 \
            g++-12 \
            libtcmalloc-minimal4 \
            libnuma-dev \
            lsof
          
          # Set gcc-12 as default
          sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12

      - name: Setup ccache
        if: env.SKIP_VLLM_CPU_TESTS != 'true'
        uses: hendrikmuhs/ccache-action@4687d037e4d7cf725512d9b819137a3af34d39b3 # v1.2.15
        with:
          key: vllm-cpu-${{ matrix.python }}-${{ inputs.vllm_version || 'v0.6.4.post1' }}
          max-size: 5G

      - name: Install dependencies
        if: env.SKIP_VLLM_CPU_TESTS != 'true'
        run: |
          python -m pip install --upgrade pip
          python -m pip install tox tox-gh>=1.2

      - name: Set vLLM version and build flags
        if: env.SKIP_VLLM_CPU_TESTS != 'true'
        run: |
          echo "VLLM_VERSION=${{ inputs.vllm_version || 'v0.6.4.post1' }}" >> $GITHUB_ENV
          echo "FORCE_REBUILD=${{ inputs.force_rebuild || 'false' }}" >> $GITHUB_ENV
          echo "CACHE_DIR=${HOME}/.cache/granite-io/vllm-cpu" >> $GITHUB_ENV

      - name: Cache vLLM CPU wheel
        if: env.SKIP_VLLM_CPU_TESTS != 'true'
        uses: actions/cache@6849a6489940f00c2f30c0fb92c6274307ccb58a # v4.1.2
        with:
          path: ~/.cache/granite-io/vllm-cpu
          key: vllm-cpu-wheel-${{ env.VLLM_VERSION }}-${{ runner.os }}-${{ matrix.python }}-${{ hashFiles('scripts/build-vllm-cpu.sh') }}
          restore-keys: |
            vllm-cpu-wheel-${{ env.VLLM_VERSION }}-${{ runner.os }}-${{ matrix.python }}-
            vllm-cpu-wheel-${{ env.VLLM_VERSION }}-${{ runner.os }}-

      - name: Build and test vLLM CPU wheel
        if: env.SKIP_VLLM_CPU_TESTS != 'true'
        run: |
          # Make script executable
          chmod +x scripts/build-vllm-cpu.sh
          
          # Run the build script
          echo "Building vLLM CPU wheel..."
          WHEEL_PATH=$(scripts/build-vllm-cpu.sh)
          echo "Built wheel: $WHEEL_PATH"
          
          # Test basic functionality
          echo "Testing wheel functionality..."
          python -m pip install "$WHEEL_PATH"
          
          # Basic import test
          python -c "
          import vllm
          print(f'vLLM version: {vllm.__version__}')
          
          # Test LoRA support
          from vllm.lora import LoRARequest
          print('✓ LoRA support available')
          
          # Test guided generation support
          from vllm.sampling_params import SamplingParams
          sp = SamplingParams()
          if hasattr(sp, 'guided_regex') or 'guided_regex' in SamplingParams.__init__.__code__.co_varnames:
              print('✓ Guided generation support available')
          else:
              print('✗ Guided generation support not found')
              exit(1)
          
          print('✓ All required features are available')
          "

      - name: Download embedding model for tests
        if: env.SKIP_VLLM_CPU_TESTS != 'true'
        run: |
          pip install -U "huggingface_hub[cli]" 
          pip install hf_xet
          huggingface-cli download sentence-transformers/multi-qa-mpnet-base-dot-v1

      - name: Run CPU vLLM unit tests
        if: env.SKIP_VLLM_CPU_TESTS != 'true'
        run: |
          # Run CPU-specific tests
          tox -e vllm-cpu-unit -- -v -k "not (e2e or slow)" tests/
        timeout-minutes: 30

      - name: Test RAG notebook with CPU vLLM
        if: env.SKIP_VLLM_CPU_TESTS != 'true'
        run: |
          # Test the RAG notebook which uses LoRA and guided generation
          # Note: We'll need to modify the notebook to use smaller models/parameters for CI
          echo "Testing RAG notebook compatibility..."
          
          # Create a minimal test script based on the RAG notebook
          cat > test_rag_cpu.py << 'EOF'
          #!/usr/bin/env python3
          """Test RAG functionality with CPU vLLM"""
          
          import os
          import tempfile
          from granite_io.backend.vllm_server import LocalVLLMServer
          from granite_io import make_io_processor
          
          # Test with a small model for CI
          model_name = "microsoft/DialoGPT-small"  # Small model for testing
          
          try:
              # Test LocalVLLMServer initialization
              print("Testing LocalVLLMServer initialization...")
              server = LocalVLLMServer(
                  model_name,
                  # Use minimal settings for CI
                  max_model_len=512,
                  enable_lora=True,
              )
              print("✓ LocalVLLMServer created successfully")
              
              # Test backend creation
              backend = server.make_backend()
              print("✓ Backend created successfully")
              
              # Test IO processor
              io_proc = make_io_processor(model_name, backend=backend)
              print("✓ IO processor created successfully")
              
              # Test guided generation parameters
              from granite_io.io.granite_3_3.input_processors.granite_3_3_input_processor import Granite3Point3Inputs
              test_input = Granite3Point3Inputs.model_validate({
                  "messages": [{"role": "user", "content": "Hello"}],
                  "generate_inputs": {
                      "temperature": 0.0,
                      "max_tokens": 10,
                      "guided_regex": r"Hello.*"
                  }
              })
              print("✓ Guided generation parameters accepted")
              
              print("All CPU vLLM tests passed!")
              
          except Exception as e:
              print(f"Test failed: {e}")
              import traceback
              traceback.print_exc()
              exit(1)
          finally:
              if 'server' in locals():
                  try:
                      server.shutdown()
                  except:
                      pass
          EOF
          
          python test_rag_cpu.py
        timeout-minutes: 15

      - name: Upload build logs on failure
        if: failure() && env.SKIP_VLLM_CPU_TESTS != 'true'
        uses: actions/upload-artifact@84480863f228bb9747b473957fcc9e309aa96097 # v4.4.2
        with:
          name: vllm-cpu-build-logs-${{ matrix.python }}
          path: |
            /tmp/vllm-cpu-build/
            ~/.cache/granite-io/vllm-cpu/build_info.json
          retention-days: 7

  report-status:
    name: "Report CPU vLLM test status"
    runs-on: ubuntu-latest
    needs: test-cpu-vllm
    if: always()
    steps:
      - name: Report results
        run: |
          if [[ "${{ needs.test-cpu-vllm.result }}" == "success" ]]; then
            echo "✅ CPU vLLM tests passed successfully"
          elif [[ "${{ needs.test-cpu-vllm.result }}" == "skipped" ]]; then
            echo "⏭️ CPU vLLM tests were skipped (likely due to missing AVX512 support)"
          else
            echo "❌ CPU vLLM tests failed"
            exit 1
          fi
