{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of the Granite certainty intrisic\n",
    "\n",
    "This notebook shows the usage of the IO processor for the Granite certainty intrisic, \n",
    "also known as the [Granite 3.3 8B Instruct Uncertainty LoRA](\n",
    "    https://huggingface.co/ibm-granite/granite-3.3-8b-rag-agent-lib/blob/main/certainty_lora/README.md\n",
    ")\n",
    "\n",
    "This notebook can run its own vLLM server to perform inference, or you can host the \n",
    "models on your own server. \n",
    "\n",
    "To use your own server, set the `run_server` variable below\n",
    "to `False` and set appropriate values for the constants \n",
    "`openai_base_url`, `openai_base_model_name` and `openai_lora_model_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go here\n",
    "from granite_io.io.granite_3_3.input_processors.granite_3_3_input_processor import (\n",
    "    Granite3Point3Inputs,\n",
    ")\n",
    "from granite_io import make_io_processor, make_backend\n",
    "from granite_io.backend.vllm_server import LocalVLLMServer\n",
    "from granite_io.io.certainty import CertaintyIOProcessor, CertaintyCompositeIOProcessor\n",
    "from granite_io.io.rag_agent_lib import obtain_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "base_model_name = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "lora_model_name = \"certainty\"\n",
    "\n",
    "run_server = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_server:\n",
    "    # Start by firing up a local vLLM server and connecting a backend instance to it.\n",
    "    # Download and cache the model's LoRA adapter.\n",
    "    lora_model_path = obtain_lora(lora_model_name)\n",
    "    print(f\"Local path to LoRA adapter: {lora_model_path}\")\n",
    "    server = LocalVLLMServer(\n",
    "        base_model_name, lora_adapters=[(lora_model_name, lora_model_path)]\n",
    "    )\n",
    "    server.wait_for_startup(200)\n",
    "    lora_backend = server.make_lora_backend(lora_model_name)\n",
    "    backend = server.make_backend()\n",
    "else:  # if not run_server\n",
    "    # Use an existing server.\n",
    "    # Modify the constants here as needed.\n",
    "    openai_base_url = \"http://localhost:55555/v1\"\n",
    "    openai_api_key = \"granite_intrinsics_1234\"\n",
    "    openai_base_model_name = base_model_name\n",
    "    openai_lora_model_name = lora_model_name\n",
    "    backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_base_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_lora_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example chat completion with a user question and two documents.\n",
    "chat_input = Granite3Point3Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"assistant\", \"content\": \"Welcome to pet questions!\"},\n",
    "            {\"role\": \"user\", \"content\": \"Which of my pets have fleas?\"},\n",
    "        ],\n",
    "        \"documents\": [\n",
    "            {\"doc_id\": 1, \"text\": \"My dog has fleas.\"},\n",
    "            {\"doc_id\": 2, \"text\": \"My cat does not have fleas.\"},\n",
    "        ],\n",
    "        \"generate_inputs\": {\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 4096,\n",
    "        },\n",
    "    }\n",
    ")\n",
    "chat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the example input through Granite 3.3 to get an answer\n",
    "granite_io_proc = make_io_processor(\"Granite 3.3\", backend=backend)\n",
    "result = await granite_io_proc.acreate_chat_completion(chat_input)\n",
    "result.results[0].next_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the model's output to the chat\n",
    "next_chat_input = chat_input.with_next_message(result.results[0].next_message)\n",
    "next_chat_input.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the I/O processor for the certainty intrinsic\n",
    "io_proc = CertaintyIOProcessor(lora_backend)\n",
    "\n",
    "# Set temperature to 0 because we are not sampling from the intrinsic's output\n",
    "next_chat_input = next_chat_input.with_addl_generate_params({\"temperature\": 0.0})\n",
    "\n",
    "# Pass our example input through the I/O processor and retrieve the result\n",
    "chat_result = await io_proc.acreate_chat_completion(next_chat_input)\n",
    "\n",
    "print(\n",
    "    f\"Certainty score for the original response is \"\n",
    "    f\"{chat_result.results[0].next_message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with an artifical poor-quality assistant response.\n",
    "from granite_io.types import AssistantMessage\n",
    "\n",
    "chat_result_2 = await io_proc.acreate_chat_completion(\n",
    "    chat_input.with_next_message(\n",
    "        AssistantMessage(content=\"Your iguana is absolutely covered in fleas.\")\n",
    "    ).with_addl_generate_params({\"temperature\": 0.0})\n",
    ")\n",
    "print(\n",
    "    f\"Certainty score for the low-quality response is \"\n",
    "    f\"{chat_result_2.results[0].next_message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use majority voting to get a second opinion\n",
    "from granite_io.io.voting import MBRDMajorityVotingProcessor\n",
    "\n",
    "voting_proc = MBRDMajorityVotingProcessor(io_proc)\n",
    "next_chat_input.generate_inputs.temperature = 0.1\n",
    "chat_result_3 = await voting_proc.acreate_chat_completion(\n",
    "    next_chat_input.with_addl_generate_params({\"n\": 10})\n",
    ")\n",
    "print(\n",
    "    f\"Certainty score with majority voting is \"\n",
    "    f\"{chat_result_3.results[0].next_message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the composite processor to generate multiple completions and filter those that\n",
    "# are below a certainty threshold\n",
    "composite_proc = CertaintyCompositeIOProcessor(\n",
    "    granite_io_proc, lora_backend, threshold=0.8, include_score=True\n",
    ")\n",
    "composite_results = await composite_proc.acreate_chat_completion(\n",
    "    chat_input.with_addl_generate_params({\"n\": 5, \"temperature\": 1.0})\n",
    ")\n",
    "composite_results.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the certainty threshold and try again\n",
    "composite_proc.update_threshold(0.9)\n",
    "composite_results_2 = await composite_proc.acreate_chat_completion(\n",
    "    chat_input.with_addl_generate_params({\"n\": 5, \"temperature\": 1.0})\n",
    ")\n",
    "composite_results_2.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU resources\n",
    "if \"server\" in locals():\n",
    "    server.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
